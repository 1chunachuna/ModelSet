#这是随机森林二分类的代码，二分类试过了，主要是四分类，需要设出四分类的阈值

train_data=pd.read_csv(r"E:\大创\train新版.txt",sep=' ',header=None)
test_data=pd.read_csv(r"E:\大创\test完整.txt",sep=' ',header=None)
data = train_data.drop(train_data.columns[[26]],axis=1)
data1 = test_data.drop(test_data.columns[[26]],axis=1)
new_column_names = ['0','1','x1','x2','x3','x4','x5','x6','x7','x8','x9','x10','x11','x12','x13','x14', 'x15', 'x16', 'x17','x18','x19','x20','x21','x22','x23', 'x24','y']# 替换成你想要的新列名
data.columns = new_column_names
data1.columns = new_column_names
print(data)


data = data.drop(data.columns[[4]], axis=1)
data = data.drop(data.columns[[4]], axis=1)
data = data.drop(data.columns[[7]], axis=1)
data = data.drop(data.columns[[11]], axis=1)
data = data.drop(data.columns[[16]], axis=1)
data = data.drop(data.columns[[17]], axis=1)
data = data.drop(data.columns[[17]], axis=1)
data = data.drop(data.columns[[0]], axis=1)
#如果去掉时间，则还需运行如下代码
#data = data.drop(data.columns[[0]], axis=1)

data.iloc[:, -1] = np.where(data.iloc[:, -1] > 125, 125, data.iloc[:, -1])
import numpy as np
from sklearn.preprocessing import MinMaxScaler
# 选取前 13 列的数据
data_to_normalize = data.iloc[:,0:-1]
# 创建 MinMaxScaler
scaler = MinMaxScaler()
# 调用 fit_transform 进行归一化
normalized_data = scaler.fit_transform(data_to_normalize)
# 将归一化后的数据替换回原数据中的对应位置
data.iloc[:, 0:-1] = normalized_data
print(data)


import numpy as np
data1 = data1.drop(data1.columns[[4]], axis=1)
data1 = data1.drop(data1.columns[[4]], axis=1)
data1 = data1.drop(data1.columns[[7]], axis=1)
data1 = data1.drop(data1.columns[[11]], axis=1)
data1 = data1.drop(data1.columns[[16]], axis=1)
data1 = data1.drop(data1.columns[[17]], axis=1)
data1 = data1.drop(data1.columns[[17]], axis=1)
data1 = data1.drop(data1.columns[[0]], axis=1)
#如果去掉时间，则还需运行如下代码
#data1 = data1.drop(data1.columns[[0]], axis=1)

data1.iloc[:, -1] = np.where(data1.iloc[:, -1] > 125, 125, data1.iloc[:, -1])
import numpy as np
from sklearn.preprocessing import MinMaxScaler
# 选取前 13 列的数据
data_to_normalize = data1.iloc[:, 0:-1]
# 创建 MinMaxScaler
scaler = MinMaxScaler()
# 调用 fit_transform 进行归一化
normalized_data = scaler.fit_transform(data_to_normalize)
# 将归一化后的数据替换回原数据中的对应位置
data1.iloc[:, 0:-1] = normalized_data
print(data1)

from imblearn.over_sampling import SMOTE  
from sklearn.ensemble import RandomForestClassifier  
from sklearn.model_selection import train_test_split  
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix, accuracy_score
import numpy as np
from sklearn.preprocessing import MinMaxScaler
# 假设x_train和y_train是你的训练数据和标签，x_test和y_test是你的测试数据和标签  


def classify_remaining_life(remaining_life):
    if remaining_life <=40:
        return 0
    else:
        return 1


X_train, y_train = data.iloc[:, 0:-1].values, data.iloc[:, -1].values
X_test, y_test = data1.iloc[:, 0:-1].values, data1.iloc[:, -1].values
scaler = MinMaxScaler()
# 调用 fit_transform 进行归一化
X_train = scaler.fit_transform(X_train)
X_test = scaler.fit_transform(X_test)
# 确保一致的数据类型
X_train = X_train.astype(float)
X_test = X_test.astype(float)

# 使用分类函数转换剩余寿命标签
y_train = np.array([classify_remaining_life(rl) for rl in y_train])
y_test = np.array([classify_remaining_life(rl) for rl in y_test])

# 初始化 SMOTE
smote = SMOTE(sampling_strategy='auto', random_state=42)

# 对训练数据应用 SMOTE
x_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# 检查形状
print(x_train_smote.shape, y_train_smote.shape)

rf = RandomForestClassifier(class_weight='balanced', random_state=42)
rf.fit(x_train_smote, y_train_smote)

# 调整阈值（如果是四分类或者三分类需要查询阈值设定问题）
threshold = 0.10
y_pred_prob = rf.predict_proba(X_test)[:, 1]
y_pred = (y_pred_prob > threshold).astype(int)

# 输出分类报告
print(classification_report(y_test, y_pred))
conf_matrix = confusion_matrix(y_test,y_pred)

# 打印混淆矩阵
print("Confusion Matrix:")
print(conf_matrix)
accuracy = accuracy_score(y_test,y_pred)

print(f'分类准确率: {accuracy * 100:.2f}%')